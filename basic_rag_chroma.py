# # -------------------------------
# # ğŸ§  STEP 3 â€” Persistent RAG System with ChromaDB
# # -------------------------------

# from dotenv import load_dotenv
# import os
# from pathlib import Path

# # Load environment variables
# load_dotenv()

# if os.getenv("GROQ_API_KEY"):
#     print("Groq API Key loaded:", os.getenv("GROQ_API_KEY")[:8], "********")
# else:
#     print("âš ï¸ GROQ_API_KEY not found! Please add it to your .env file.")

# # -------------------------------
# # LangChain imports
# # -------------------------------
# from langchain_groq import ChatGroq
# from langchain_huggingface import HuggingFaceEmbeddings
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.vectorstores import Chroma
# from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
# from langchain.chains import RetrievalQA

# # -------------------------------
# # Step 1 â€” Load PDFs from data folder
# # -------------------------------
# data_folder = Path(__file__).parent / "data"
# data_folder = data_folder.resolve()

# print(f"ğŸ“‚ Loading all PDFs from: {data_folder}")

# loader = DirectoryLoader(str(data_folder), glob="**/*.pdf", loader_cls=PyPDFLoader)
# documents = loader.load()
# print(f"âœ… Loaded {len(documents)} pages from all PDFs.")

# # -------------------------------
# # Step 2 â€” Split into text chunks
# # -------------------------------
# print("ğŸ”¹ Splitting text into manageable chunks...")
# splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
# chunks = splitter.split_documents(documents)
# print(f"âœ… Split into {len(chunks)} text chunks.")

# # -------------------------------
# # Step 3 â€” Create or load ChromaDB vector store
# # -------------------------------
# print("ğŸ§  Setting up persistent ChromaDB...")

# chroma_dir = Path(__file__).parent / "chroma_db"
# chroma_dir.mkdir(exist_ok=True)

# embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# # Create (or reuse existing) vector database
# db = Chroma.from_documents(
#     documents=chunks,
#     embedding=embeddings,
#     persist_directory=str(chroma_dir)
# )

# db.persist()  # saves database to disk
# print(f"ğŸ’¾ ChromaDB created at: {chroma_dir}")

# # Create retriever
# retriever = db.as_retriever(search_kwargs={"k": 3})

# # -------------------------------
# # Step 4 â€” Connect to Groq LLM
# # -------------------------------
# llm = ChatGroq(model="llama-3.1-8b-instant", groq_api_key=os.getenv("GROQ_API_KEY"))

# qa = RetrievalQA.from_chain_type(
#     llm=llm,
#     retriever=retriever,
#     return_source_documents=True
# )

# # -------------------------------
# # Step 5 â€” Ask Questions
# # -------------------------------
# while True:
#     query = input("\nğŸ¤– Ask a question (or type 'exit' to quit): ")
#     if query.lower() in ["exit", "quit"]:
#         print("ğŸ‘‹ Goodbye!")
#         break

#     result = qa.invoke({"query": query})

#     print("\n-----------------------------")
#     print("ğŸ” QUESTION:", query)
#     print("-----------------------------")
#     print("ğŸ’¬ ANSWER:", result['result'])
#     print("-----------------------------")

#     # Show sources
#     for i, doc in enumerate(result["source_documents"], start=1):
#         print(f"\nğŸ“„ Source {i}: {doc.metadata['source']} (Page {doc.metadata.get('page', '?')})")
#         print(doc.page_content[:200], "...")









# -------------------------------
# ğŸ§  STEP 4.5 â€” Conversational RAG System with Memory + ChromaDB
# -------------------------------

from dotenv import load_dotenv
import os
from pathlib import Path

# Load environment variables
load_dotenv()

if os.getenv("GROQ_API_KEY"):
    print("Groq API Key loaded:", os.getenv("GROQ_API_KEY")[:8], "********")
else:
    print("âš ï¸ GROQ_API_KEY not found! Please add it to your .env file.")

# -------------------------------
# Imports
# -------------------------------
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader

# ğŸ”¹ NEW imports for conversational memory
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# -------------------------------
# Step 1 â€” Load PDFs from data folder
# -------------------------------
data_folder = Path(__file__).parent / "data"
data_folder = data_folder.resolve()

print(f"ğŸ“‚ Loading all PDFs from: {data_folder}")

loader = DirectoryLoader(str(data_folder), glob="**/*.pdf", loader_cls=PyPDFLoader)
documents = loader.load()
print(f"âœ… Loaded {len(documents)} pages from all PDFs.")

# -------------------------------
# Step 2 â€” Split into text chunks
# -------------------------------
print("ğŸ”¹ Splitting text into manageable chunks...")
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
chunks = splitter.split_documents(documents)
print(f"âœ… Split into {len(chunks)} text chunks.")

# -------------------------------
# Step 3 â€” Setup persistent ChromaDB
# -------------------------------
chroma_dir = Path(__file__).parent / "chroma_db"
chroma_dir.mkdir(exist_ok=True)

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

print("ğŸ§  Loading or creating ChromaDB...")
db = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory=str(chroma_dir)
)
db.persist()
print(f"ğŸ’¾ ChromaDB ready at: {chroma_dir}")

retriever = db.as_retriever(search_kwargs={"k": 3})

# -------------------------------
# Step 4 â€” LLM + Conversational Memory Setup
# -------------------------------
llm = ChatGroq(model="llama-3.1-8b-instant", groq_api_key=os.getenv("GROQ_API_KEY"))

# ğŸ§  Create conversational memory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# ğŸ”— Conversational Retrieval Chain combines memory + retriever + LLM
qa = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory
)

# -------------------------------
# Step 5 â€” Multi-turn Conversation
# -------------------------------
print("\nğŸ¤– Conversational RAG System Ready! Ask me anything (type 'exit' to quit).")

while True:
    query = input("\nğŸ§‘ You: ")
    if query.lower() in ["exit", "quit"]:
        print("ğŸ‘‹ Goodbye! Chat history saved in memory (in runtime only).")
        break

    result = qa.invoke({"question": query})

    print("\nğŸ’¬ Assistant:", result["answer"])
